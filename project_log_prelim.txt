

Functionality:


		ETLETLETLETLELTELTELTETLETLETETETLETLETLETLETETLETL


The data is collected using etl_meteo.py. The script contains a list of arbitrary location
coordinates for which weather data is collected. 

Using the function "request_new_weather_data" accopmplishes the following:

	- The destination folder, "source_to_raw", is cleansed from any earlier data.

	- Using a for-loop, each location in the list of locations gets input into a 
	querystring together with the selected parameters. The parametes are har coded and 
	are currently hourly data over the next 4 days for temperature, relative humidity
	and precipitation.

	- Using the querystring, data is pulled from OM.

	- Using datetime and the location coordinates, the file is saved (json) and named as
	the time of request as well as the location of the forecast.

The above is true for each of the locations, and they will be saved as separate files.

Using the function "transform", accomplishes the following:

	- The destination folder, "raw_to_harmonized", is cleansed from any previous data.

	- As the raw data contains unnecessary information which not only wastes space, but
 	also makes any attempt at usage of the data extremely difficult, it is transformed.

	- The function pulls files from "source_to_raw", makes a dataframe out of the part
	Â¨that is needed (['hourly']).

	- The data is then saved to the destination folder as a json.

Using the function "rename", accomplishes the following:

	-Each file inte the directory, "raw_to_harmonized" is looped through and checked if
 	the file name ends with a certain set of coordnates, and depending on which
 	coordinates, it is renamed after the actual town or city corresponding to the
 	coordinates.
 
	-Note that all of this is hard coded and would need changing if the list of
 	locations should be updated.

		DDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDBDB

The python script handling the database insertion is called simons_databasgrej.py.

Using sqlalchemy, an engine is created inorder to connect to the postgres database called 
"weather". In this database the are four tables, each corresponding to a location. The columns
in the tables correspond to the dataframe columns. The corresponding table names are stored
in a list.

Using the "insert_data_to_dbs", accomplishes the following:

	- Each file in the folder "raw_to_harmonized" is looped through.

	- First, the name of the location is extracted from the file name.

	- Then the json is converted into a dataframe.
	
	- The previously extracted location name is added as a column.
	This is unnecessary in the current database format, but is kept
	in case another database structure should be used.

	-It then loops through each table name, finding the one matching
	the current json-file. Again, using the extracted location. 
	
	-The data is then inserted into the corresponding table.


		GRPHGRHPRPHGRPHGRPHGRPHGRPHGRPHGRPHGRPHGRPHGRPH

The python script handing the graph creation is called 	too_easy_simon.py.

Using the function "plot_weather" accomplishes the following:

	- A loop pulls database names from the list of databases.
	- Using each database name, the desired data is pulled from the database using psycopg2
	- The data is plotted using matplotlib.
	- The destination folder is checked for duplicates and these are then deleted.
	- The plotted graphs are saved as png in the destination folder.


		DAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAGDAG

The python script handling the DAG procedure is called my_dag.py.

The above functions are pulled from the scripts, sequenced and executed.


